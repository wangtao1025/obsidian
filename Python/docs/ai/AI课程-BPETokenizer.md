# AI 课程：BPE Tokenizer

本文对应课程 [rag.docs-hub.com](https://rag.docs-hub.com/) 中的 **BPETokenizer** 文档，文档归在 **AI 分类**。BPE（Byte Pair Encoding）是子词分词算法，用于解决「词表大小 vs 覆盖度」的两难：词级词典大且易 OOV，字符级序列过长；BPE 通过迭代合并高频相邻符号得到子词词表，被 GPT 等大模型广泛采用。可与 [大语言模型（llm）](/ai/AI课程-llm)、[text_splitter](/python/AI课程-text_splitter) 搭配理解 Token 与分词。

---

## 一、核心思想

- **词级**：词表巨大，未登录词（OOV）无法表示。
- **字符级**：词表极小、可表示任意串，但单 token 信息少、序列过长、效率低。
- **BPE**：基于**子词**，常见词/词组成独立 token，生僻词拆成有意义的子词或字符，兼顾词表规模与覆盖度。

---

## 二、BPE 是什么？

BPE 源自数据压缩：找**最常共现的字节对**，用新符号替换（Byte Pair → Encoding）。在 NLP 中把「字节」换成字符或子词，迭代合并**出现频率最高的相邻符号对**，逐步扩大词表。

---

## 三、工作流程：训练 + 编码

### 阶段一：训练（从语料学习合并规则）

1. **初始化**：按字符切分每个词，词尾加结束符 `</w>`；初始词表 = 所有字符。
2. **迭代合并**：统计所有相邻符号对频率 → 选频率最高的一对 → 合并为新符号并加入词表 → 重复直到达到预定合并次数或词表大小。
3. **输出**：按顺序排列的**合并规则列表**（如 `e s -> es`，`es t -> est`）及最终词表，即训练好的 BPE 模型。

### 阶段二：编码（对新文本分词）

1. 预处理：按空格分词、加 `</w>`。
2. 按字符拆开。
3. **按训练得到的顺序**依次应用合并规则，直到无法再合并。
4. 得到子词 token 序列。

---

## 四、优势与特点

- **效率与覆盖度平衡**：常见词成整 token，罕见词拆成子词。
- **缓解 OOV**：未见过的新词可拆成已知子词（如「ChatGPT」→ Chat + GPT）。
- **序列长度**：介于词级与字符级之间，压缩效果好。
- **语言无关**：不依赖空格等特定语言规则，对黏着语、屈折语等也适用。

---

## 五、实际应用中的细节

- **GPT 系列**：使用 **Byte-level BPE**，以字节为基元，可处理任意文本。
- **SentencePiece**：开源实现 BPE 及 Unigram 等，不依赖预分词，对中文、日文等更友好。
- **WordPiece（BERT）**：与 BPE 类似，但合并时按**频率与语言模型似然**综合选对，而非仅频率。

---

**相关文档**：[大语言模型（llm）](/ai/AI课程-llm) · [text_splitter](/python/AI课程-text_splitter) · [OpenAI](/python/AI课程-openai) · [知识体系与学习路径](/ai/知识体系与学习路径) · [BPETokenizer（课程原文）](https://rag.docs-hub.com/html/BPETokenizer.html)
