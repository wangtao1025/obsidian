# AI 课程笔记：RAG 评估与 RAGAs

来源课程：[rag.docs-hub.com](https://rag.docs-hub.com/html/)（zhangdocs）第五讲。以下为 RAG 系统评估维度、流程与 RAGAs 评测指标（Context Precision、Faithfulness、Answer Relevance、Context Recall）的整理。

---

## 一、RAG 系统评估的意义

在 AI 产品开发中，通常先做出基线版本，再通过测试与反馈迭代。对 RAG 而言，**科学的评估**既是衡量优劣的依据，也是后续改进的方向。系统性评估「提问—检索—生成」各环节的表现，是项目落地的核心环节。

---

## 二、RAG 评估的核心维度

RAG 流程可抽象为：**提问 → 检索 → 生成**。评估围绕这三步之间的关联展开。

### 2.1 Context Relevance（检索内容的相关性）

- **关注点**：检索到的资料是否**贴合用户问题**，是否包含解答所需的关键信息。
- **理想情况**：检索结果紧扣问题主题、具备针对性和实用性。  
- **示例**：问「如何预防高血压？」时，应检索到饮食、运动等预防措施，而非泛泛的健康常识。

### 2.2 Faithfulness（生成答案的事实一致性）

- **关注点**：生成答案是否**忠实于检索到的资料**，事实是否有据可查，是否凭空捏造或与资料矛盾。
- **理想情况**：答案中的每个事实都能在上下文中找到支持。
- **示例**：若资料说「应减少盐分摄入」，答案却说「多吃甜食」，则事实不一致。

### 2.3 Answer Relevance（答案与问题的直接相关性）

- **关注点**：答案是否**直接回应问题**、是否完整、是否夹杂无关或冗余信息。
- **理想情况**：针对问题主题、完整回答核心诉求、简洁无跑题。
- **示例**：问「高血压的常见症状有哪些？」时，答案应列举症状，而非大谈成因或治疗。

### 2.4 案例分析（课程表）

| 项目 | 内容 |
|------|------|
| 用户问题 | 请简述地球自转的影响。 |
| 资料 A | 地球自转导致昼夜交替，并影响全球风系分布。 → **与问题高度相关** |
| 资料 B | 太阳系中有八大行星，地球是其中之一。 → **与问题无关** |
| 生成答案 1 | 地球自转使得地球表面出现昼夜变化，还影响了风的流向。 → **与问题高度相关，与资料 A 事实一致** ✓ |
| 生成答案 2 | 地球是太阳系的第三颗行星。 → **未回答自转影响，与资料 B 一致但不回答问题** ✗ |

### 2.5 评估方式

- **人工评估**：精确但耗时、成本高。  
- **自动化评估**：借助 **LLM** 自动判断相关性、事实一致性等，在保证一定准确性的同时提升效率，已成为常用方案。

---

## 三、评估流程（三阶段）

1. **准备测试集**：一批有代表性的问题 + 每个问题对应的**权威参考答案**（如标准医学解答）。  
2. **明确指标**：采用通用 RAG 指标（如下文四类），或按业务细化为准确率、相关性、覆盖度等。  
3. **执行评测**：将测试集问题输入 RAG → 得到检索上下文与生成答案 → 用评测工具对比标准答案，计算各指标得分，为优化提供数据支撑。

---

## 四、RAG 评测工具：RAGAs

**RAGAs**（2023 年发布）面向 RAG 的**自动化评测**，目标是在少依赖人工标注的前提下评估系统表现。后续版本中部分指标也支持引入**人工标准答案**以提升可靠性。RAGAs 利用 **LLM** 的推理能力，综合考察**问题、检索上下文、生成答案**三者之间的关系并计算指标。

---

### 4.1 Context Precision（上下文精度）

#### 4.1.1 核心定义

- 衡量**检索返回的上下文列表**中，与当前问题**真正相关的文档**所占比例及**排序质量**。  
- 既看「有多少相关文档」，也看「相关文档是否排在前面」。

#### 4.1.2 计算公式与步骤

对每个查询，设检索返回 K 个片段：

- \( rel_k \)：第 k 个片段是否相关（1=相关，0=不相关）。
- **前 k 个片段的精度**：\( P@k = \frac{前 k 个中相关的数量}{k} \)。
- **Context Precision**：
  \[
  \text{Context Precision} = \frac{\sum_{k=1}^{K} (P@k \times rel_k)}{\text{总相关片段数}}
  \]
  若总相关片段数为 0，则 Context Precision = 0。

#### 4.1.3 计算示例（课程表）

| 位置 k | 片段 | 是否相关 relₖ | 前 k 个相关数量 | P@k | P@k × relₖ |
|--------|------|----------------|------------------|-----|------------|
| 1 | Doc A | 1 | 1 | 1.0 | 1.0 |
| 2 | Doc B | 0 | 1 | 0.5 | 0.0 |
| 3 | Doc C | 1 | 2 | 0.67 | 0.67 |
| 4 | Doc D | 0 | 2 | 0.5 | 0.0 |
| 5 | Doc E | 1 | 3 | 0.6 | 0.6 |

- 总相关片段数 = 3。  
- 分子 = 1.0 + 0 + 0.67 + 0 + 0.6 = 2.27。  
- **Context Precision = 2.27 / 3 ≈ 0.7567**。

#### 4.1.4 设计逻辑

- **奖励排序**：相关文档越靠前，P@k 越大，贡献越大。  
- **惩罚不相关**：不相关文档会拉低后续 P@k。  
- **归一化**：除以总相关数，使不同查询之间可比。

#### 4.1.5 极端情况

- **完美检索**（相关全在前）：如 [1,1,1,0,0] → 得分为 1.0。  
- **最差检索**（相关全在后）：如 [0,0,0,1,1] → 得分约 0.325。  
- **混合**：如 [1,0,1,0,1] → 得分约 0.757。

#### 4.1.6 在 RAG 中的意义

- **高**：检索准且排序好，有利于 LLM 利用前文生成高质量答案。  
- **低**：检索不准和/或排序差，需优化检索与排序策略。

---

### 4.2 Faithfulness（忠实性）

#### 4.2.1 核心定义

- 生成答案与**检索上下文**的**事实一致性**。  
- 忠实答案应：所有声明在上下文中有支持、不捏造、不扭曲或过度解读。

#### 4.2.2 计算方法（三步）

1. **从生成答案中提取声明**：将答案拆成可单独验证的事实陈述。  
   - 例：「爱因斯坦在 1905 年发表狭义相对论，并因此获诺贝尔奖。」→ 声明 1「发表狭义相对论」、声明 2「1905 年」、声明 3「因狭义相对论获诺贝尔奖」。
2. **验证每个声明的支持性**：在上下文中判断——完全支持 / 部分支持 / 不支持 / 矛盾。
3. **计算得分**：  
   \[
   \text{Faithfulness} = \frac{\text{被支持的声明数}}{\text{总声明数}}
   \]

#### 4.2.3 计算示例

- **上下文**：「爱因斯坦于 1905 年提出狭义相对论，其中包含质能方程 E=mc²。」  
- **生成答案**：「爱因斯坦在 1905 年提出狭义相对论，其中包含质能方程 E=mc²，这是他获得诺贝尔奖的主要贡献。」  
- 声明 1、2：完全支持；声明 3（诺贝尔奖）：上下文未提及 → 不支持。  
- **Faithfulness = 2/3 ≈ 0.67**。

---

### 4.3 Answer Relevance（回答相关性）

#### 4.3.1 核心定义

- 衡量生成答案**在多大程度上直接回答原问题**（不在此指标中考虑事实正确性）。  
- 关注：**相关性**（是否针对问题）、**完整性**（是否答全）、**简洁性**（是否少无关信息）。

#### 4.3.2 计算方法

**方法一：基于 LLM 的评分**

- 用 Prompt 让 LLM 对「答案对问题的相关性」打 1–5 分，再标准化到 0–1：  
  **标准化分数 = (评分 - 1) / 4**。  
- 评分标准概要：5=完全直接回答、完整无冗余；4=基本回答、略有不足；3=部分相关、有遗漏或冗余；2=少量相关；1=完全不相关或回避问题。

**方法二：基于相似度**

- 用句子向量模型（如 `all-MiniLM-L6-v2`）对**问题**和**生成答案**编码，计算**余弦相似度**作为相关性分数。

```python
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity

def calculate_answer_relevance_similarity(question, answer):
    model = SentenceTransformer('all-MiniLM-L6-v2')
    question_embedding = model.encode([question])
    answer_embedding = model.encode([answer])
    similarity = cosine_similarity(question_embedding, answer_embedding)[0][0]
    return similarity
```

#### 4.3.3 示例（课程）

- **高度相关**：问「Python 的主要特点？」答「简洁易读、动态类型、解释型、丰富标准库、跨平台」→ 5 分 → 1.0。  
- **部分相关**：只提到「有很多库、适合数据科学」、遗漏语法与类型等 → 3 分 → 0.5。  
- **低相关**：答「编程语言有很多种，Java、C++ 都很流行」→ 2 分 → 0.25。

---

### 4.4 Context Recall（上下文召回率）

#### 4.4.1 核心定义

- 衡量**检索到的上下文**中，包含了**回答问题所需全部相关信息**的比例。  
- 关注检索的**覆盖度、完整性、召回能力**。

#### 4.4.2 计算方法

- 基于**人工标注的黄金标准**：先确定「回答该问题所需的所有相关片段」集合。  
- 公式：  
  \[
  \text{Context Recall} = \frac{\text{检索到的相关片段数}}{\text{黄金标准中相关片段总数}}
  \]  
  或按「被检索到的相关片段 / 全部相关片段」理解。  
- **不相关片段**不参与分子分母，只算「该找的有没有找到」。

#### 4.4.3 计算步骤

1. **建立黄金标准**：人工标出所有相关片段。  
2. **运行检索**：得到 RAG 实际返回的上下文。  
3. **计算召回率**：检索结果与黄金标准的重叠比例。

#### 4.4.4 示例

- 黄金标准 5 个相关片段：{A, B, C, D, E}。  
  - 检索到 {A,B,C,D,E} → Recall = 1.0。  
  - 检索到 {A,C,E} → Recall = 0.6。  
  - 检索到 {A,B,X,Y,Z}（X,Y,Z 不相关）→ 只计 A,B → Recall = 2/5 = 0.4。

#### 4.4.5 基于 LLM 的自动化

- 用 LLM 根据**问题 + 参考答案**，析出「关键信息点」，再判断这些点在**检索上下文**中出现了多少，计算「出现数 / 总关键信息点数」作为召回率近似。

---

## 五、小结与参考

- **四个常用 RAGAs 指标**：  
  - **Context Precision**：检索相关性与排序质量。  
  - **Faithfulness**：答案是否忠实于检索上下文。  
  - **Answer Relevance**：答案是否直接、完整、简洁地回答问题。  
  - **Context Recall**：检索是否覆盖了应有相关信息的比例。  
- 实际使用时：准备测试集与（可选）标准答案 → 选定指标 → 跑 RAG 并调用 RAGAs 或自研脚本计算各指标，据此迭代检索与生成模块。

课程内关联：[RAG 检索增强生成](/ai/AI课程-RAG检索增强生成)（第四讲）、[Milvus 向量数据库与 PyMilvus](/ai/AI课程-Milvus向量数据库与PyMilvus)、[RAG 与向量基础](/ai/AI课程-RAG与向量基础)；[RAGAS](/ai/AI课程-RAGAS) 框架详见本站单篇，[LangChain](/ai/AI课程-LangChain) 见本站 [AI课程-LangChain](/ai/AI课程-LangChain)，其余见课程导航与前述笔记。

---

*（本文档为课程第五讲「RAG 评估」的整理，与第四讲「RAG」衔接。）*
