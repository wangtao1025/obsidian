# AI 课程：MMR（最大边际相关性）

本文对应课程 [rag.docs-hub.com](https://rag.docs-hub.com/) 中的 **MMR** 文档，归在 **AI 分类**（检索与排序）。MMR（Maximal Marginal Relevance）在保证**相关性**的同时最大化**多样性**，用于检索结果去重、推荐多样化、文档摘要选句等。可与 [RAG 与向量基础](/ai/AI课程-RAG与向量基础)、[余弦相似度](/ai/AI课程-余弦相似度)、[LangChain](/ai/AI课程-LangChain)（MaxMarginalRelevanceExampleSelector）搭配学习。

---

## 1. 什么是 MMR？为什么需要？

- **MMR**：最大边际相关性，在返回结果中既保证与查询相关，又让结果之间尽量不重复、不冗余。
- **为什么需要**：纯按相关性排序可能返回多篇内容高度相似的文档（如都讲同一事件）；MMR 通过平衡「与查询相关」和「与已选文档不同」，得到更全面、多样化的结果。

---

## 2. 核心思想与公式

- **两个目标**：**相关性**（与查询 Q 相关）、**多样性**（与已选集合 S 中的文档不同）。
- **平衡方式**：MMR 分数 = λ × Sim(D, Q) − (1−λ) × max Sim(D, D_j)，D_j ∈ S。  
  - 前者越大越好（相关），后者越大越惩罚（与已选相似），通过 λ 调节权重。
- **符号**：D 候选文档，Q 查询，S 已选集合，λ 权衡参数（0～1）。

---

## 3. 参数 λ 的作用

| λ 取值 | 倾向 | 说明 |
|--------|------|------|
| λ = 1 | 完全相关性 | 等同按相关性排序，可能很冗余 |
| λ = 0 | 完全多样性 | 只考虑彼此不同，可能偏离查询 |
| λ ≈ 0.5 | 平衡 | 相关性与多样性权重相当 |
| λ ≈ 0.7 | 略偏相关性 | 常用设置，检索/推荐中常用 |

**选参建议**：信息检索 0.5～0.7；文档摘要 0.3～0.5；推荐系统 0.6～0.8。

---

## 4. 算法步骤（贪心）

1. 计算所有候选文档与查询 Q 的相关性（如 [余弦相似度](/ai/AI课程-余弦相似度)）。  
2. **第一轮**：选与 Q 最相关的文档加入 S（无已选文档时无法算多样性）。  
3. **后续轮**：对每个未选文档 D 计算 MMR 分数（λ×Sim(D,Q) − (1−λ)×与已选文档的最大相似度），选分数最高的加入 S。  
4. 重复直到选满 k 个文档。

**特点**：贪心策略，实现简单、速度快，但不保证全局最优；复杂度约 O(kn²)。

---

## 5. 应用场景与优缺点

| 场景 | 说明 |
|------|------|
| **检索结果多样化** | 搜索引擎、RAG 检索后对 top 结果做 MMR 重排，避免多条相似。 |
| **推荐系统** | 在相关推荐中插入多样性，避免推荐过于同质。 |
| **文档摘要** | 从长文中选句时，用 MMR 使选中句子覆盖不同方面。 |

- **优点**：逻辑清晰、可调 λ、通用、能明显减冗余。  
- **缺点**：贪心非全局最优、n 大时计算贵、需调参、依赖相似度质量。  
- **适用**：需要多样化且 k 不太大（如 k < 20）；不适合仅要相关性排序或 n 极大、实时要求极高的场景。

---

## 6. 小结

- **MMR**：用 λ 平衡「与查询相关」和「与已选不同」，贪心选 k 个文档。  
- **实现**：先算与 Q 的相似度，首条选最相关，后续按 MMR 公式选最大分加入 S。  
- **在 RAG/LangChain**：[LangChain](/ai/AI课程-LangChain) 的 `MaxMarginalRelevanceExampleSelector` 即用 MMR 在初选结果上再做多样性选择。

---

**相关文档**：[RAG 与向量基础](/ai/AI课程-RAG与向量基础) · [余弦相似度](/ai/AI课程-余弦相似度) · [LangChain（LCEL 与链）](/ai/AI课程-LangChain) · [词袋模型（BagofWords）](/ai/AI课程-BagofWords) · [知识体系与学习路径](/ai/知识体系与学习路径) · [MMR（课程原文）](https://rag.docs-hub.com/html/MMR.html)
